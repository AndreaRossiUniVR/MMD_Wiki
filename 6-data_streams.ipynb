{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining data streams\n",
    "\n",
    "In this notebook, rather than presenting the different tools for analyzing a data stream, we discuss a practical use case that requires the use of such tools.\n",
    "\n",
    "We will not suggest any specific library (e.g., implementation of the Bloom filter), so the student has to search and choose the ones that best suit the project requirements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caches, and cache hit ratio\n",
    "\n",
    "We consider a Web cache, i.e., a server with a specific amount of memory $M$, that intercepts the user requests for Web pages (items) and:\n",
    "\n",
    "- if the item is not in the cache, it retrieves it from the origin server, replies to the user, and decides if caching such an item;\n",
    "- if the item is already stored in the cache, it replies directly to the user.\n",
    "\n",
    "<br>\n",
    "<img src=https://appcheck-ng.com/wp-content/uploads/Web-Cache-Diagram.png width=\"300\">\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "Since the memory is limited, the cache needs to decide which items should be stored. This can be formalized with a **cache mananagement policy**, that decides the item *admission* and *eviction*.\n",
    "\n",
    "The admission regulates the decision related to a requested item that is not in the cache. The eviction regulates which item should be removed in case there is no storage space left and a new item must be stored.\n",
    "\n",
    "The choice of which policy should be adopted is driven by the effect on a performance index, the **hit ratio**:\n",
    "\n",
    "$$\n",
    "\\mbox{Hit ratio} = \\frac{\\mbox{requests satisfied by the cache}}{\\mbox{received requests}}\n",
    "$$\n",
    "\n",
    "A well-known cache management policy is **LRU** (Least Frequently Used). This policy maintains the list of stored items sorted by their access time. In case of a request for an item already stored in the cache, there is a *hit* and the item is moved to the front of the sorted list. In case of a request for an item not stored in the cache, there is a *miss*, the item then is placed at the front of the sorted list, and the last item in the list is removed. \n",
    "\n",
    "Note that, in case of heterogeneous item sizes, more than one items could be removed to make room for the new item -- conversely, a single item may free a lager space than necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic LRU cache implementation\n",
    "\n",
    "We assume that each item is associated to a kay-value pair, where the *key* is the item identifier and the *value* is the item itself. \n",
    "A cache server usually exposes two interfaces (also called API): ```get(key)``` and  ```set(key, value)```. The former is used to query the cache: if the item (identified by that key) is stored in the cache, it is returned, otherwise nothing is returned. the latter is used to store an item to the cache.\n",
    "\n",
    "Let's create a class that simulate the behavior of a cache. We will use ```OrderedDict```, which is a special implementation of a dictionary in which the insertion order is maintained. In addition, we will use the parameter *value* when storing an item for storing the item size, rather than the item itself (this is a simulation of a cache, not a real cache)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class LRUCache:\n",
    "\n",
    "    def __init__(self, max_size = 100):\n",
    "        if max_size <= 0:\n",
    "            raise ValueError\n",
    "        self.items = OrderedDict()\n",
    "        self.max_size = max_size\n",
    "        self.curr_size = 0\n",
    "\n",
    "    def get(self, key):\n",
    "        if key not in self.items:\n",
    "            return None\n",
    "        self.items.move_to_end(key)\n",
    "        return self.items[key]\n",
    "\n",
    "    # Note that the parameter \"value\" is used to \n",
    "    # store the item size, not the actual item\n",
    "    def set(self, key, value=1):\n",
    "        if value <= 0:\n",
    "            raise ValueError\n",
    "        self.items[key] = value\n",
    "        self.items.move_to_end(key)\n",
    "        self.curr_size += value\n",
    "        evicted = []\n",
    "        while self.curr_size > self.max_size:\n",
    "            rem_k, rem_v = self.items.popitem(last=False)\n",
    "            evicted.append(rem_k)\n",
    "            self.curr_size -= rem_v\n",
    "        return evicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item request trace \n",
    "\n",
    "Portions of data streams can be recorded into a file so that they can be replayed for testing purpose (e.g., to check if a different caching management policy produces higher hit ratios). We will use one trace collected from a real-world Web cache. The format is very simple: each line contains the item that has been requested, i.e., its key and the size of the item returned. See for example:\n",
    "\n",
    "```\n",
    "0 310\n",
    "1 2324\n",
    "2 74\n",
    "3 19076\n",
    "4 31061\n",
    "5 817\n",
    "```\n",
    "\n",
    "The trace contains more than **11 Millions** requests.\n",
    "\n",
    "We will read this trace, store the items into a LRU cache, and compute the hit ratio over time (at regular intervals). We start with a simple case in which we do not consider the item size.\n",
    "\n",
    "*Note that we read directly the gzipped file, without the need to unzip it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "# input parameters\n",
    "input_file = \"6-ak1vot3_255t60_trace.txt.gz\"\n",
    "cache_max_size = 10000\n",
    "max_num_req = 1000000\n",
    "delta_req = 10000 # register the hit ratio at regular intervals\n",
    "\n",
    "# other variables\n",
    "hits = 0.0\n",
    "hit_ratio_vs_time = []\n",
    "cache = LRUCache(cache_max_size)\n",
    "\n",
    "# open the trace and read it line by line\n",
    "with gzip.open(input_file, 'r') as f_trace:\n",
    "    for num_req, line in enumerate(f_trace):\n",
    "        # stop reading the file if you reached the max num of reqs\n",
    "        if max_num_req > 0 and num_req > max_num_req:\n",
    "            break\n",
    "        splitted_line = line.split()\n",
    "        item_id = int(splitted_line[0])\n",
    "        # for the moment, we do not consider the item size\n",
    "        # item_size = int(splitted_line[1])\n",
    "        item_size = 1\n",
    "        \n",
    "        # check if the item is in the cache\n",
    "        val = cache.get(item_id)\n",
    "        if val is None:\n",
    "            evicted = cache.set(item_id, item_size)\n",
    "        else:\n",
    "            hits += 1.0\n",
    "        \n",
    "        # for the graph\n",
    "        if num_req > 0 and num_req%delta_req == 0:\n",
    "            hit_ratio_vs_time.append(hits/num_req)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the hit ratio over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(np.arange(delta_req, num_req, delta_req), hit_ratio_vs_time)\n",
    "plt.xlabel('Num. req.')\n",
    "plt.ylabel('Hit ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q1\n",
    "<div class=\"alert alert-info\">\n",
    "Analyze the general characteristics of the trace, such as:\n",
    "    \n",
    "- Exact number of requests;\n",
    "- Number of distinct items;\n",
    "- Average item size, and item size empirical distribution;\n",
    "- Popularity empirical distribution (how many times each item is requested, sorted by most to least popular);\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q2\n",
    "<div class=\"alert alert-info\">\n",
    "Change the cache size and observe its impact on the hit ratio. What happens when the cache is extremely small (e.g., 100)? And when it is extremely large? \n",
    "    \n",
    "<br>\n",
    "\n",
    "Note: A graph that contains the value of the cumulative hit ratio for different cache sizes is called \"Hit Ratio Curve (HRC)\", and its complement to 1 is called \"Miss Ratio Curve (MRC)\". The MRC is  a common tool used to characterize a cache management policy. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q3\n",
    "<div class=\"alert alert-info\">\n",
    "Introduce the item size (and adjust the cache size accondingly) and observe the impact of the cache size on the hit ratio.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open problem P1\n",
    "\n",
    "When we consider the hit ratio, we keep track of the cumulative number of hits over the comulative number of requests. This *averaging effect* may hide some temporal fluctuation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "How can we measure the instantaneous hit ratio?\n",
    "</div>\n",
    "\n",
    "There could be different solutions to this problem, but we do not know which one works best, so we should compare all of them.\n",
    "\n",
    "The options are:\n",
    "\n",
    "- RESET: considering an interval of $R$ requests, count the number of hits in that interval (each interval is independent);\n",
    "- SLIDING: consider a sliding window of the last $R$ requests and update the hit ratio accordingly;\n",
    "- DECAYING: use an exponentially decaying window instead of a sliding window.\n",
    "\n",
    "The three policies has different parameters, so one should also study the impact of such parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open problem P2\n",
    "\n",
    "Looking closely at the statistics of the trace, it is possible to notice that many items are requested just once. This phenomenon is not specific to this trace, but it can be found in many real-world traces, and it is usually referred to as *one-hit wonders*.\n",
    "\n",
    "One-hit wonders may be problematic, since they waste space: in fact, they enter in the LRU cache (causing the eviction of other items), but they do not generate any hit. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "How can we efficiently estimate the number (or the percentage) of one-hit wonders in a stream?\n",
    "</div>\n",
    "\n",
    "The solution of the above problem implies that we estimate how many distinct items the stream contains, and among them which items have more than one request. This may require the use of multiple tools.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Can we design a policy that exclude one-hit wonders from the cache?\n",
    "</div>\n",
    "\n",
    "Unfortunately it is not possible to known in advance which requests will be one-hit wonders, so any policy will have an impact on all the items. This means that the gain obtained from excluding the one-hit wonders could be counterbalanced by the loss due to the regular items. But we can know it only if we try!\n",
    "\n",
    "The task will be to design a new policy, which we will call 2-STEPS, that will admit a new item only if it is the second time that is has been requested.\n",
    "\n",
    "```python\n",
    "class 2-STEPS:\n",
    "\n",
    "    def __init__(self, max_size):\n",
    "        # same as LRU, with some additional data structure\n",
    "\n",
    "    def get(self, key):\n",
    "        # same as LRU\n",
    "\n",
    "    def set(self, key, value):\n",
    "        # this must be change to reflect the admission policy;\n",
    "        # eviction is the same as LRU\n",
    "\n",
    "```\n",
    "\n",
    "*Hint:* We need to keep track in an **efficient** way the item seen so far; but we should also avoid the data structure saturation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
